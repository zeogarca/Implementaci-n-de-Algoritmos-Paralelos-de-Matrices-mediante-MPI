## Multiplicaci√≥n de Matrices

![image](https://github.com/user-attachments/assets/39931cd0-8393-4ce7-bf8d-894a2bf9519a)


# Multiplicaci√≥n de Matrices con MPI y OpenMP

## ¬øQu√© hace el programa?

Este proyecto realiza la multiplicaci√≥n de dos matrices cuadradas usando programaci√≥n paralela. Utiliza dos tecnolog√≠as:

- MPI (Message Passing Interface) para repartir el trabajo entre varios procesos n√∫cleos.

- OpenMP para aprovechar varios hilos dentro de cada proceso.

La combinaci√≥n de ambas permite que el programa sea r√°pido y eficiente, aprovechando al m√°ximo los recursos disponibles.

## ¬øC√≥mo est√° organizado el programa?

El programa est√° dividido en dos tipos de procesos:

- Maestro (rank 0): es el que organiza todo. Crea las matrices A y B, reparte partes de ellas entre los dem√°s procesos, y al final recoge los resultados.

- Esclavos (los dem√°s procesos): reciben sus partes de las matrices y hacen su parte del c√°lculo. Luego env√≠an el resultado de vuelta al maestro.

## Pasos del programa

Inicio y lectura del tama√±o de la matriz:

1. El usuario le dice al programa qu√© tama√±o deben tener las matrices cuadradas (por ejemplo, 1000x1000).

- 2. Se inicializa MPI para que todos los procesos se preparen.

- Creaci√≥n y reparto de datos (por el maestro)

Se crean dos matrices aleatorias: A y B.

La matriz A se divide por filas, y la matriz B se divide por columnas.

Cada proceso recibe una parte de A y una parte de B, seg√∫n lo que le toca.

C√≥mputo (todos los procesos)

Cada proceso multiplica las filas que le tocaron de A por las columnas de B que le llegaron.

Para esto se usa OpenMP, que permite que dentro de cada proceso se usen varios hilos para hacer los c√°lculos m√°s r√°pido.

Recolecci√≥n de resultados

Cada proceso genera un pedazo de la matriz resultado C.

El maestro junta todos esos pedazos para armar la matriz completa.

## Final

Se imprime cu√°nto tiempo tard√≥ la operaci√≥n.

Se libera la memoria usada y se termina el programa.




### ‚ùå Mejoras en la siguiente entrega

| √Årea         | Descripci√≥n                                              | Mejora Propuesta                                                            |
|--------------|----------------------------------------------------------|-----------------------------------------------------------------------------|
| **Memoria**      | Cada proceso guarda una copia completa de B.             | Distribuir B por bloques columnares.                                       |

## Resultados Actuales

              lgarcia@ce:~/m1$ mpirun -np 4 ./matrix_mpi_openmp 2000
              
              ‚úÖ Execution time: 0.375169 seconds
              lgarcia@ce:~/m1$ mpirun -np 8 ./matrix_mpi_openmp 2000
              
              ‚úÖ Execution time: 0.457550 seconds
              lgarcia@ce:~/m1$ mpirun -np 16 ./matrix_mpi_openmp 2000
              
              ‚úÖ Execution time: 0.547565 seconds
              lgarcia@ce:~/m1$ mpirun -np 24 ./matrix_mpi_openmp 2000
              
              ‚úÖ Execution time: 0.605839 seconds

## Resultado en Numpy

              Resultados:
              Tama√±o de las matrices: 2000x2000
              Tiempo de ejecuci√≥n: 0.4345 segundos

## Proceso maestro:

- Crear las matrices completas (A y B) y llenarlas con valores.

- Dividir el trabajo (distribuir filas de A entre los procesos).

- Recolectar los resultados parciales para armar la matriz resultado C.

- Mostrar el tiempo de ejecuci√≥n y (opcionalmente) las matrices.

## Procesos esclavos:

- Reciben su parte de filas de la matriz A para multiplicar por la matriz B.

- Ejecutan la multiplicaci√≥n s√≥lo sobre su pedazo asignado.

- Env√≠an de vuelta los resultados parciales al maestro.

## üß† Inicializaci√≥n

      MPI_Init(&argc, &argv);
      MPI_Comm_rank(MPI_COMM_WORLD, &rank);
      MPI_Comm_size(MPI_COMM_WORLD, &size);
      Se inicializa MPI.

rank: el n√∫mero de proceso actual.

size: cantidad total de procesos.

## üì¶ Distribuci√≥n de datos
1. El proceso 0 crea las matrices:

        if (rank == 0) {
            A = malloc(...);
            C = malloc(...);
            fill_matrix(A, N);
            fill_matrix(B, N);
        }
   
Se generan dos matrices A y B con valores aleatorios.

A se va a distribuir, y B se transmite completa a todos los procesos.

2. Se calcula cu√°ntas filas le toca a cada proceso:

        rows_per_proc = N / size;
        remaining_rows = N % size;
        local_rows = rows_per_proc + (rank < remaining_rows ? 1 : 0);
   
Esto se hace para manejar matrices que no se dividen exactamente entre los procesos.

El offset calcula desde qu√© fila empieza cada proceso.

3. Se usa MPI_Scatterv para enviar solo las filas necesarias de A a cada proceso:

        MPI_Scatterv(A, sendcounts, displs, MPI_DOUBLE, A_local, ..., 0, MPI_COMM_WORLD);

sendcounts y displs indican cu√°ntos elementos enviar y desde qu√© posici√≥n.


