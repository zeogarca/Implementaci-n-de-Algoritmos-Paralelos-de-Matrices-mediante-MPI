# ‚öôÔ∏è Estructura General del C√≥digo
1. Inicializaci√≥n y entrada
Se obtiene N (tama√±o de la matriz) desde los argumentos del programa.

Se inicializa MPI, se identifican rank (n√∫mero de proceso) y size (n√∫mero total de procesos).

2. Divisi√≥n de trabajo
Se calcula cu√°ntas filas de la matriz A le corresponden a cada proceso (sendcounts, displs).

Se reparte A entre procesos con MPI_Scatterv.

B se transmite completa a todos los procesos con MPI_Bcast.

3. Multiplicaci√≥n local
Cada proceso realiza la multiplicaci√≥n de su parte de A con toda B, almacenando el resultado en local_C.

Se utiliza OpenMP para paralelizar el ciclo anidado de multiplicaci√≥n.

4. Recolecci√≥n de resultados
Se utiliza MPI_Gatherv para juntar todas las partes de C en el proceso ra√≠z (rank 0).

El tiempo de ejecuci√≥n se mide desde el proceso 0 con MPI_Wtime.

5. Finalizaci√≥n
Se imprimen las matrices si son peque√±as (N <= 16) y el tiempo total de ejecuci√≥n.

Se libera memoria y se finaliza MPI.

#üìä Ventajas de la Implementaci√≥n Actual
‚úÖ Aprovecha paralelismo a dos niveles: entre procesos (MPI) y entre hilos (OpenMP).

‚úÖ Usa Scatterv y Gatherv, permitiendo una distribuci√≥n equilibrada aunque N no sea divisible entre procesos.

‚úÖ Puede correr en sistemas distribuidos y aprovechar m√∫ltiples n√∫cleos.

#‚ùå Limitaciones y Oportunidades de Mejora
√Årea	Descripci√≥n	Mejora Propuesta
Memoria	Cada proceso guarda una copia completa de B.	Distribuir B por bloques columnares.
Cach√©	Multiplicaci√≥n no optimizada para cach√©.	Usar tiling (blocking) para mejorar la localidad de memoria.
OpenMP	Usa solo collapse(2) sin pol√≠tica de balanceo.	Agregar schedule(dynamic) para mejor balanceo.
Modularidad	Todo el c√≥digo est√° en main.	Separar en funciones (distribuir_matrices(), multiplicar_local(), etc).
Validaci√≥n	No se compara el resultado con un m√©todo secuencial.	Agregar comparaci√≥n contra resultado secuencial en rank 0.
Reutilizaci√≥n	Repetici√≥n de mallocs y liberaci√≥n de memoria.	Crear funciones auxiliares para asignaci√≥n y liberaci√≥n.
Escalabilidad	Podr√≠a saturar nodos con demasiados procesos MPI.	Usar un modelo h√≠brido bien balanceado (1 MPI por nodo + OpenMP por core).
